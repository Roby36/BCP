{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web3 import Web3\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from web3 import AsyncWeb3\n",
    "from web3.exceptions import Web3RPCError\n",
    "from web3.providers.persistent import WebSocketProvider\n",
    "import asyncio\n",
    "import threading\n",
    "from time import sleep\n",
    "from typing import List\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Router contract using etherscan API\n",
    "def fetch_abi(address: str) -> list:\n",
    "    ETHERSCAN_API_KEY = \"QGS1VSEPNEHZ8W4M686QCNM1Z6WGIT14Q1\"\n",
    "    url = (\n",
    "      \"https://api.etherscan.io/api\"\n",
    "      f\"?module=contract&action=getabi&address={address}\"\n",
    "      f\"&apikey={ETHERSCAN_API_KEY}\"\n",
    "    )\n",
    "    resp = requests.get(url).json()\n",
    "    return json.loads(resp[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. set up RPC w3 connection\n",
    "api_keys = [\n",
    "    \"75db4a2f907d4525866a728681b3b458\",\n",
    "    \"d3c123079991433a9117637ed32fc540\",\n",
    "    \"056abf38c1fb4da8b3dcff03d9a32d3c\",\n",
    "    \"9ef3857c438e4f3485bb10daf32fdfa9\",\n",
    "    \"b461cd0decb4477396db1e04244bb1ee\",\n",
    "    \"8c2befadd58e43329df6331f5b4fa609\"\n",
    "]\n",
    "\n",
    "api_key = api_keys[3]\n",
    "\n",
    "infura_url = f\"https://mainnet.infura.io/v3/{api_key}\" # select mainnet or sepolia here\n",
    "w3 = Web3(Web3.HTTPProvider(infura_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping track & serializing all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "PAIR_STATES_PATH    = \"/Volumes/Extreme SSD/arbot_data/pair_states.json\"\n",
    "PAIR_ABI_PATH = \"/Volumes/Extreme SSD/arbot_data/pair_abi.json\"\n",
    "DEPLOY_BLOCK = 10_008_355\n",
    "sleep_time: float = 1.0\n",
    "pair_created_topic  = w3.keccak(text=\"PairCreated(address,address,address,uint256)\")\n",
    "sync_topic          = w3.keccak(text=\"Sync(uint112,uint112)\")\n",
    "factory_address_string = \"0x5C69bEe701ef814a2B6a3EDD4B1652CB9cc5aA6f\"\n",
    "factory_address = Web3.to_checksum_address(factory_address_string)\n",
    "factory_abi     = fetch_abi(factory_address)\n",
    "factory         = w3.eth.contract(address=factory_address, abi=factory_abi)\n",
    "with open(PAIR_ABI_PATH) as f:\n",
    "    pair_abi = json.load(f)\n",
    "\n",
    "# TODO: Will have to be scoped inside appropriate thread for safety\n",
    "pair_states: dict[str, dict] = {}\n",
    "pair_states_last_block = 0\n",
    "pair_states_lock    = asyncio.Lock()\n",
    "state_subscribed    = asyncio.Event()\n",
    "initial_update      = asyncio.Event()\n",
    "\n",
    "\n",
    "# (0) Functions to read/write from disk\n",
    "\n",
    "def load_pair_states():\n",
    "    global pair_states_last_block, pair_states\n",
    "    if os.path.isfile(PAIR_STATES_PATH):\n",
    "        with open(PAIR_STATES_PATH) as f:\n",
    "            data = json.load(f)\n",
    "            pair_states_last_block = data.get(\"pair_states_last_block\", DEPLOY_BLOCK - 1)\n",
    "            pair_states      = data.get(\"pair_states\", {})\n",
    "    else:\n",
    "        pair_states_last_block = DEPLOY_BLOCK - 1\n",
    "        pair_states      = {}\n",
    "\n",
    "async def dump_pair_states():\n",
    "    global pair_states_last_block, pair_states\n",
    "    tmp = PAIR_STATES_PATH + \".tmp\"\n",
    "    payload = {\n",
    "        \"pair_states_last_block\": pair_states_last_block,\n",
    "        \"pair_states\":     pair_states\n",
    "    }\n",
    "    with open(tmp, \"w\") as f:\n",
    "        json.dump(payload, f)\n",
    "    os.replace(tmp, PAIR_STATES_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event Handlers\n",
    "def _handle_pair_creation(log) -> bool:\n",
    "    global pair_states\n",
    "    \"\"\" \n",
    "    Called back on each new PairCreated event \n",
    "    \"\"\"\n",
    "    \n",
    "    # Ignore log if it did not come from our factory\n",
    "    # TODO: in future accept from multiple factories\n",
    "\n",
    "    if log[\"address\"].lower() != factory_address_string.lower():\n",
    "        return False\n",
    "    \n",
    "    decoded_log = factory.events.PairCreated().process_log(log)\n",
    "    addr = decoded_log[\"args\"][\"pair\"]\n",
    "    if addr not in pair_states:\n",
    "        pair_states[addr] = {\n",
    "            \"pairAddress\": addr,\n",
    "            \"factory\": log[\"address\"],\n",
    "            \"token0\": decoded_log[\"args\"][\"token0\"],\n",
    "            \"token1\": decoded_log[\"args\"][\"token1\"],\n",
    "            \"blockNumber\": log[\"blockNumber\"]\n",
    "        }\n",
    "        return True\n",
    "        #Â print(f\"\\tAppended pair {addr} (total now {len(pair_states)})\")\n",
    "    return False\n",
    "\n",
    "def _handle_synch(log) -> bool:\n",
    "    global pair_states, pair_abi\n",
    "    \"\"\" \n",
    "    Called back on each Synch event \n",
    "    \"\"\"\n",
    "    addr = log[\"address\"]\n",
    "    if addr not in pair_states:\n",
    "        return False # drop Sync for unknown pools\n",
    "    decoded_log = w3.eth.contract(address=addr, abi=pair_abi).events.Sync().process_log(log)\n",
    "    r0, r1 = decoded_log[\"args\"][\"reserve0\"], decoded_log[\"args\"][\"reserve1\"]\n",
    "    blk = log[\"blockNumber\"]\n",
    "\n",
    "    # Only maintain very latest state in this batch\n",
    "    if blk >= pair_states[addr][\"blockNumber\"]:\n",
    "        pair_states[addr].update({\n",
    "            \"reserves\": {\n",
    "                \"reserve0\": str(r0),\n",
    "                \"reserve1\": str(r1),\n",
    "                \"lastBlockTimestamp\": blk,\n",
    "            },\n",
    "            \"blockNumber\": blk\n",
    "        })\n",
    "        return True\n",
    "        # print(f\"\\tSync @ {blk} for {addr}: {r0}/{r1}\")\n",
    "    return False\n",
    "\n",
    "log_sort_key = lambda L: (\n",
    "        L[\"blockNumber\"],\n",
    "        L[\"transactionIndex\"],\n",
    "        L[\"logIndex\"],\n",
    "    )\n",
    "\n",
    "async def safe_get_logs(args: dict):\n",
    "    from_blk    = args[\"fromBlock\"]\n",
    "    to_blk      = args[\"toBlock\"]\n",
    "    topics      = args[\"topics\"]\n",
    "\n",
    "    try:\n",
    "        # controlling requests to the node\n",
    "        await asyncio.sleep(sleep_time)\n",
    "        return w3.eth.get_logs(args)\n",
    "    except Web3RPCError as e:\n",
    "        if \"more than 10000\" in str(e):\n",
    "            mid = (from_blk + to_blk) // 2\n",
    "            logs1 = await safe_get_logs(args={\n",
    "                \"fromBlock\": from_blk,\n",
    "                \"toBlock\": mid,\n",
    "                \"topics\": topics\n",
    "            })\n",
    "            logs2 = await safe_get_logs(args={\n",
    "                \"fromBlock\": mid + 1,\n",
    "                \"toBlock\": to_blk,\n",
    "                \"topics\": topics,\n",
    "            })\n",
    "            return sorted(logs1 + logs2, key=log_sort_key)\n",
    "        raise\n",
    "         \n",
    "async def pair_states_log_range(from_blk: int, to_blk: int) -> int:\n",
    "    global pair_states, pair_states_last_block, pair_abi\n",
    "\n",
    "    logs = await safe_get_logs({\n",
    "        \"fromBlock\": from_blk,\n",
    "        \"toBlock\":   to_blk,\n",
    "        \"topics\":    [[pair_created_topic, sync_topic]]\n",
    "    })\n",
    "\n",
    "    print(f\"Fetched {len(logs)} logs\")\n",
    "\n",
    "    # sort by (block, tx, log index)\n",
    "    logs.sort(key=log_sort_key)\n",
    "\n",
    "    # 1. Track creation logs and latest Sync for each address\n",
    "    creation_logs = []\n",
    "    latest_sync = {}\n",
    "    for log in logs:\n",
    "        topic_sig = log[\"topics\"][0]\n",
    "        if topic_sig == pair_created_topic:\n",
    "            creation_logs.append(log)\n",
    "        elif topic_sig == sync_topic:\n",
    "            addr = log[\"address\"]\n",
    "            key = (log[\"blockNumber\"], log[\"transactionIndex\"], log[\"logIndex\"])\n",
    "            prev = latest_sync.get(addr)\n",
    "            if prev is None or key > prev[0]:\n",
    "                latest_sync[addr] = (key, log)\n",
    "    \n",
    "    # 2. First handle all newly created pairs\n",
    "    new_pairs = 0\n",
    "    for log in creation_logs:\n",
    "        new_pairs += _handle_pair_creation(log)\n",
    "    \n",
    "    # 3. Then handle exactly one sync per address\n",
    "    for _, (_, log) in latest_sync.items():\n",
    "        _handle_synch(log)\n",
    "\n",
    "    # 4. Finally save to disk\n",
    "    pair_states_last_block = to_blk\n",
    "    async with pair_states_lock:\n",
    "        await dump_pair_states()\n",
    "\n",
    "    print(f\"Processed blocks {from_blk}-{to_blk}: +{new_pairs} pools, total {len(pair_states)}\")\n",
    "    return new_pairs\n",
    "\n",
    "\n",
    "async def initial_pair_states_sync(chunk_size: int = 50_000):\n",
    "    \"\"\" \n",
    "    Synchronizes pair_states up to a given block\n",
    "    \"\"\"\n",
    "    global pair_states, pair_states_last_block\n",
    "\n",
    "    load_pair_states()\n",
    "    await state_subscribed.wait()\n",
    "    to_block = w3.eth.block_number\n",
    "\n",
    "    start = pair_states_last_block + 1\n",
    "    for blk in range(start, to_block + 1, chunk_size):\n",
    "        end = min(blk + chunk_size - 1, to_block)\n",
    "        await pair_states_log_range(blk, end)\n",
    "    \n",
    "    initial_update.set()\n",
    "\n",
    "\n",
    "async def monitor_state():\n",
    "    global pair_states_last_block\n",
    "\n",
    "    async with AsyncWeb3(WebSocketProvider(f\"wss://mainnet.infura.io/ws/v3/{api_key}\")) as w3s:\n",
    "        sub_id = await w3s.eth.subscribe(\"logs\", {\n",
    "            \"topics\": [[pair_created_topic, sync_topic]]\n",
    "        })\n",
    "        state_subscribed.set()\n",
    "\n",
    "        async for msg in w3s.socket.process_subscriptions():\n",
    "            await initial_update.wait()\n",
    "            log = msg[\"result\"]\n",
    "\n",
    "            topic_sig = log[\"topics\"][0]\n",
    "            if topic_sig == pair_created_topic:\n",
    "                print(\"Live Pair Created Event: \")\n",
    "                _handle_pair_creation(log)\n",
    "            elif topic_sig == sync_topic:\n",
    "                print(\"Live Synch Event: \")\n",
    "                _handle_synch(log)\n",
    "            \n",
    "            pair_states_last_block = log[\"blockNumber\"]\n",
    "            async with pair_states_lock:\n",
    "                await dump_pair_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrypoint: this thread runs indefinitely keeping pair_states up to date\n",
    "async def main():\n",
    "\n",
    "    await asyncio.gather(\n",
    "        initial_pair_states_sync(chunk_size=100_000),\n",
    "        monitor_state(),\n",
    "    )\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junk-cleaning procedure after bug discovery\n",
    "\"\"\"\n",
    "\n",
    "load_pair_states()\n",
    "\n",
    "with open(\"/Volumes/Extreme SSD/arbot_data/pairaddr.json\") as f:\n",
    "    pairaddr = json.load(f)\n",
    "\n",
    "pairaddr_dict = {}\n",
    "for pair in pairaddr[\"pairs\"]:\n",
    "    pairaddr_dict[pair] = pair\n",
    "\n",
    "# Only keep the items that are amongst legitimate addresses\n",
    "new_pair_states = {}\n",
    "for pair in pair_states.keys():\n",
    "    if pair in pairaddr_dict:\n",
    "        new_pair_states[pair] = pair_states[pair]\n",
    "\n",
    "# Rewrite the global pair_states dict by clearing then updating\n",
    "pair_states.clear()             \n",
    "pair_states.update(new_pair_states)\n",
    "\n",
    "await dump_pair_states()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "attempts chunk size 5000\n",
    "breaks at \n",
    "Fetched 7765 logs\n",
    "Processed blocks 10213355-10218354: +16 pools, total 651\n",
    "\n",
    "attempts chunk size 3000\n",
    "Fetched 6860 logs\n",
    "Processed blocks 10293355-10296354: +13 pools, total 1278\n",
    "\n",
    "attempts chunk size 2000\n",
    "Fetched 5589 logs\n",
    "Processed blocks 10352355-10354354: +6 pools, total 1566\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test_rand_pair():\n",
    "    test_i = random.randint(0, len(pair_addresses) - 1)\n",
    "    assert( factory.functions.allPairs(test_i).call() == pair_addresses[test_i] )\n",
    "    print(f\"Assertion passed for pair # {test_i}\")\n",
    "\n",
    "while True:\n",
    "    test_rand_pair()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to date?\n",
    "len(pair_addresses) == factory.functions.allPairsLength().call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_num = factory.functions.allPairsLength().call() - 10\n",
    "factory.functions.allPairs(curr_num).call() == pair_addresses[curr_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all pair contracts use the same abi? (looks like they do, hence save unique abi under this pair_abi object)\n",
    "pair_num = -1\n",
    "\n",
    "pair_address = Web3.to_checksum_address(pair_addresses[pair_num])\n",
    "pair_abi     = fetch_abi(pair_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PAIR_ABI_PATH, \"w\") as f:\n",
    "    json.dump(pair_abi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The proposed shape for each pair\n",
    "\n",
    "\n",
    "{\n",
    "  \"pair_addressesess\": \"0xâ¦\",            // contract address\n",
    "  \"factory\":    \"0xâ¦\",\n",
    "  \"token0\":     \"0xâ¦\",\n",
    "  \"token1\":     \"0xâ¦\",\n",
    "  \"reserves\": {\n",
    "    \"reserve0\":           \"123456789012345678\",   // as string to avoid JS precision loss\n",
    "    \"reserve1\":           \"987654321098765432\",\n",
    "    \"blockTimestampLast\": 1717065600\n",
    "  },\n",
    "  \"priceCumulatives\": {\n",
    "    \"price0CumulativeLast\": \"123456789012345\",    // price1/price0 * elapsed time\n",
    "    \"price1CumulativeLast\": \"543210987654321\"\n",
    "  },\n",
    "  \"kLast\":       \"121932631137021795113\",         // reserve0 * reserve1 at last liquidity event\n",
    "  \"totalSupply\": \"1000000000000000000\"            // LP token total supply\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Monitor single \"Synch\" event?\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
